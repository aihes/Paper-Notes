1/7 🧵 LLM 生成速度太慢？我们都习惯了模型一个词一个词地“吐字”，但这正是其效率瓶颈。

一篇名为 CALM (Continuous Autoregressive Language Models) 的论文提出了范式级转变：不再预测“下一词元”，而是预测“下一向量”！

#AI #LLM #CALM

2/7 核心思想：CALM 引入一个高保真自动编码器，将 K 个词元（tokens）无损压缩成一个连续向量。

语言模型的核心任务，从“预测下一个词”变为“预测下一个向量”，直接将生成步数减少 K 倍，从根本上打破了效率枷锁。

这是在提升每个生成步骤的“语义带宽”！

3/7 但这引出了新问题：生成目标从离散词元变为连续向量后，我们熟悉的 Softmax、交叉熵、困惑度（Perplexity）等整套工具全部失效。

怎么办？CALM 提出了一套完整、自洽的“无似然（Likelihood-Free）”工具箱来解决。

4/7 支柱 1 & 2：如何训练？

1️⃣ 鲁棒的 VAE：通过引入变分自编码器和 Dropout，确保模型对生成过程中的噪声不敏感。
2️⃣ 能量损失：一种无似然训练方法，不计算概率，而是直接衡量“生成向量”与“真实向量”之间的距离。

#DeepLearning #AIResearch

5/7 支柱 3 & 4：如何评估和采样？

3️⃣ BrierLM：一种全新的、基于经典 Brier Score 的评估指标，作为 Perplexity 的可靠替代品，解决了无似然模型的评估难题。
4️⃣ 无似然温度采样：一种精妙的拒绝采样算法，完美模拟了温度采样对生成多样性的控制。

6/7 惊人的成果：一个 371M 参数的 CALM (K=4) 模型，性能与 281M 的传统 Transformer 相当，但训练和推理所需的 FLOPs 分别减少了 44% 和 34%！

CALM 开辟了一个全新的模型缩放维度：语义带宽 K。

![Performance-Compute Trade-off](images/fiture4.png)

7/7 结论：CALM 不仅是一个新模型，更是一套迈向下一代超高效语言模型的完整框架。

它告诉我们，与其无止境地“堆料”，不如从根本上提升每个计算步骤的信息密度。这可能才是释放大模型潜力的正确道路。