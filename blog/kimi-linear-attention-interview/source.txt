发言人  
然后我觉得国内算法创新肯定是更强的线性注意的模块。他们最后选到的是一个叫做KDA的这个模块。Kimi delta attention这个名字感觉也挺有梗的，他们应该是想对标deep six fast attention，然后我就特意取了一个kimi开头的一个名字，然后非常的对照。我觉得每一次大家关心你的attention，那肯定是因为大家碰到了一些context。war. 
发言人  
我觉得我还是挺喜欢看最早的那些paper的，我觉得那些paper写的都挺好的，我管这个叫做考古。先看看能不能把全局这个注意力把它干掉。对，这是第一点，就是因为它确实它是阻止这个connect window继续scale up上去的一个主要的瓶颈。我觉得最后结合的话就是把混合注意力，它里面的全局的注意力把它换成sparse attention。我觉得理论上只要sparse attention它能选得准的话，是完全可以取代full attention这个层的。
发言人  
Hello, 大家好，欢迎收听张小军商业访谈录，我是小俊。这是一档由语言及世界工作室出品的深度访谈节目，我们希望和你一起从这里探索新世界。今天这期节目我们将讨论一个在当下非常关键的话题，那就是人工智能的算法与架构创新。嘉宾是我们的往期嘉宾返场，他是MIT的再度博士杨松林，研究方向是线性注意力机制。我们将从最近高发布的几个新模型，timeline mini max m two、queen three next切入松林参与了这其中kimi和谦问的部分工作，他是kimi linear论文的作者之一。
发言人  
那么算法创新为什么在2025年变得尤为重要呢？它背后的成因是数据、算力和算法是驱动人工智能的三驾马车。在数据撞墙的无奈前提之下，各个模型公司都不不得不重新开始雕模型架构，以期skinny load魔法技术。而由于中国的算力相对美国有限，这反而让中国的AI算法创新走在了世界的前沿。
发言人  
这期节目你将听到近几年架构最大的突破是deep sick的MOE混合专家模型，它让MOE成为了全球共识。而下一个突破的重要方向可能是attention注意力机制。中国公司已经在attention上展开了不同的技术压铸，截至目前已经发布的这些模型中，deep seat正在探索的是sparse attention稀疏注意力机制，kimi正在探索的是linear tension线性注意力机制，mini max在年初的m one版本中探索的是linear attention，而在刚发布的m two版本中又重新回到了full attention，也就是全局注意力机制。在节目中，苏宁将讲解他参与的这篇kimi linear的工作，做，并分析以上这些公司在attention上的不同抉择。与此同时他也将带领大家考古人工智能的算法变种史，并预演未来算法与架构的改进方案。
发言人  
等级比较的硬核，会有一些些的专业难度。大家可以根据自己的实际需要收听。也因为嘉宾的工作环境的原因，所以会出现一些中英文的夹杂，还是希望大家能够多多的理解和支持。我们开始。
发言人  
Hello, 宋林，先给听众朋友们打个招呼，并且做一个简单的自我介绍。Hello大家好，我叫杨聪林，我现在是MITCCL的一个PHD在读。然后我的主要研究方向的话就是那是那个model的一些架构，然后主要是比较高效的注意力机制之类的研究。更具体来说的话主要是在研究这一类注意力模型，叫做线性注意力。能不能给大家讲讲你的整个的研究的主线是怎么递进点，你是怎么走向linear attention的研究点？像的话就是最开始的时候，因为我们这边的话当时看了很多斯坦福一个research的一个group叫做hazy research，就是吹到Albert他们在斯坦福的那个lab。然后当时看了很多他们现在写的博客，然后觉得序列建模是一个非常有意思的问题。然后他们就决定来做一些系列建模的一些问题。
发言人  
然后刚刚那时候最开始读博的时候，就是微软亚演员的话，他有一篇工作叫做right night。那个时候就是最开始的时候就是想办法来提高这个runner它的效率，和它的那个performance。之后的话就发现提高效率的这一套硬件优化的这种算法，可以扩展到很多这种其他的这种类似的架构里面。然后嗯同时就之后一些工作，主要是去想办法进一步的在能够硬件高效训练的同时，能够提高这种新型注意力它加入的performance的一些改进。就比方说从门控机制，然后到有一个叫做delta road的一个机制。
发言人  
然后后面的话就是把这两个东西把它combine在一起，就是让他合成一个一个统一的一个role，然后把它变成一个安的一个更新规则。同时的话又可以去有一些可以硬件高效的算法来进行训练。我看上一次我们节目发了以后，很多人说你是linear attention之母，这是为什么？可能是在这个领域做了很多工作，尤其是还有一个开源库叫做flash linear attention。这个库的话感觉这个领域的人里面很多就是嗯会用这个库。然后包括业界也有很多用这个库来进行一些面料探索的一些探索的对，然后我那几天工作应该还是比较比较有影响力的，所以可能大家会这么来教我，您怎么更通俗的去理解一下，理解更通俗的理解。
发言人  
Linear的话就是说它中文是线性对吧？线性注意力机制。线性的话它主要的意思就是线性复杂度。对，线性复杂度它对应的话就是说是平方复杂度的，也就是我们平常的supervisor attention它是频繁复杂度。然后我们大家都知道software attention的话，它有三个矩阵，它有QK，we do query key value. 
发言人  
然后一般的话他就是Q和K先求一个矩阵相乘，得到一个LYL的一个矩阵，L的话是序列长度，然后的话就是对这个L8L的举证做一个masking。因为它基本上都是自回归的一个语言建模，所以我们要把未来的消息把它muscle，这样的话我们得到一个下三角的一个LBL的一个矩阵，然后我们再加一个soft max，然后这样我们就得到一个注意力的一个分数的一个矩阵。最后再用追离分数矩阵和那个value的矩阵做一个相乘，得到一个output。这个就是so much attention，它的这种自回归建模里面的一个比较一个粗略的介绍。对，然后因为它会有一个L8L的一个矩阵，所以它的复杂度是平方的。然后线性作业的话，他一般就是把那个software separate，把它去掉。然后这样子的话，我们就会得到就把这个非线性的so much attention的so mass去掉了。
发言人  
然后我们可以通过一些等式的变化，可以把它写成一个类似于RN的一个推理的一个形式。这样的话它每一个step，它的course就是o one。然后处理这个L它这个C长度的序列的一个画，它的整体的复杂度就是OL，所以它是跟长度的大小它是成一个线性复杂度的一个关系，所以大家会把它叫做线性注意力。
发言人  
如果把现在的大于模型的算法做一个框，让大家有一个背景的话，linear tension应该放在哪个地方？我觉得现在都在transformer这个基础架构里面再进行一些模改。对，像大语言模型的话，它的技术站可能分成post training之类的。这些架构的研究的话，那肯定是在pre train这个里面的。Pre train它还有很多其他类别的研究吗？比方说像优化器，像这种基础架构，还有一些pretend data之类的东西。然后新建作业的话，应该就算在这个基础架构的研究。
发言人  
然后现在基础架构的话，基本上它整体的这个框架，还是transformer这种他会有一个注意力及时和一个前会网络就feedforward network。他会在这两个模块里面反复的叠加，叠加很多次就得到我们最今天的一个transformer的一个architecture。对，然后一般的话，大家就是会在这个框架下面来进行一些修改。像最近几年的话，就是大家会把传统的那个MLP或者说favorite network，把它换成混合专家的一些模块，microphone expert MOE的一些模块。然后先行注意的话就是把传统的这个soft max attention把它换成一些线性复杂度的一些attention。当然就现在最近更火的，所以一类叫做hybrid的一个价格，就是有一些层它还是一个soft max attention。然后另外大部分的层就把它换成先行助力的这种层。
发言人  
对，我们来聊聊你最近参与的一个新工作，就是kimi linear。你是怎么参与到kimi linear的工作中的？这个工作应该是十月底刚发布，这个工作应该他们应该是年初就唔想开始做。然后当时就是flash linear attention这个库的另外一个主要的作者，他叫张宇。他就是今年就从博士毕业，他在国外读博，然后博士毕业，他就当时他正好就是在kimi，然后kimi就正好想做这个混合注意力，张宇就是在做这个项目。
发言人  
对，然后因为我和因为它就是FLA的那个开源库的一个collaborator，就很熟。然后我会帮他们看一下，就是有一些线性助力的一些变种，它的那些变形的算法该要什么，怎么设计之类的。当时他们团队遇到的核心问题是什么？为什么开始决定要重新设计一下注意力机制？
发言人  
年初的时候，我觉得大背景的话就是嗯像difficult I want和kimi 1.5那个时候刚刚发。对，然后他的核心的那个他会做一些RL，然后会有一些会得到一些非常长的一些思维链就transfer。然后他会用这个非常长的思维链来做这种test time scaling，对，然后来解一些比较复杂的问题。然后这个思维链的长度它往往就是能够到几万个token这个长度。然后kimi就觉得如果我们用每一层都是平方注意力的这个架构的话，他在decoding的时候他就太贵了。
发言人  
因为首先每一层他要存一个大量的一个KV cache。然后另外的话它每一步它是线性的这个时间复杂度，所以decode如果decode l个token的话，它的时间复杂度也是一个平方的对所以就是在这种长的这种思维链的生成的背景下面，让kimi觉得需要去投入资源来探索一下这种混合的注意力。因为他能够把这个inference的cost把它打低很多。对这一点在这种长丝微念non transfer这个背景下面，以及今年整体的这个agency AI的这个背景下面话，他是非常有用的。
发言人  
对，大概背景就是这样子的。你们魔改当时核心目标是什么？需要完成的核心目标。当时的核心目标可能主要是张宇在那边做吧，然后他们的目标应该跟之前的那种full attention相比的话，就是performance要不掉点。同时他的这个inference速度会快很多倍。
发言人  
如果用for attention的话，它的缺陷是什么？如果用的话就是这种做长文本的这个decoding的时候，它就是非常的昂贵，对不对？能不能从你的视角给大家讲讲这篇论文，稍微画一下重点。像这篇文章的话，他们的就是这个线性注意的模块，他们最后选到的是一个叫做KDA的这个模块。Keeping data attention. 
发言人  
对，这个名字感觉也挺有梗的。他们应该是想对标difficult attention，然后就特意取了一个kimi开头的一个名字，然后非常的对状。然后这个线性注意的模块，它是基本上就是基于我去年的有一个工作叫做getty delta。然后在这个基础上面进行了一些改善，然后最后形成了一个叫做KDA的一个模块。
发言人  
总的来说的话就是首先我们有一个叫做比亚塔路的一个东西，对，这个可能可以之后再稍微再具体讲。然后像getty的时候，就这个工作就当时收线于这个efficiency，然后当时我就用到了一个像像member two一样的一个scale value的一个getting。这个的话他就是说他的这个门控，他的这个词就是对于一个attention head来说，它下面的所有的这个维度，它全部会要共享一个decay的一个衰减率。这样的话它是可以在计算上面会带来一些简化。所以当时的考虑就是说。
发言人  
然后我先在的基础上面就是加上德塔路，让他的这个效率有保证。对然后嗯所以就会所以当时就是只用到了他们那一种比较力度比较粗的一个门控的一个机制。对，所以这就是gateway还是night。然后像张雨完的这个KDA，他就是把这个力度比较粗的一个衰减率，把它换成了一个力度比较细的一个衰减率。之前的话就是一个attention had下面他不同的维度他要共享同一个衰减率。现在的话他不同的维度，他每个维度他有一个自己的衰减率。对这样的话就是每一个维度它对应的那个RN的那个G的隐藏状态的话，它就是有自己独立的一套类更新的频率。这样的话从直觉上来看，他就是能够更好的利用这个IN有限的这个hidden state，才能够提高这个performance。
发言人  
你们的设计逻辑和灵感来源于什么？我感觉这个设计的话，其实我觉得像KTA的话，它其实就是前两个工作的一个，就把之前有两个工作的那种idea把它合并在了一起。像我之前还有一个工作叫做getty的那个attention，他就是有一个这种力度比较细的一个衰减率。然后后面到getty deadline的时候，当时之所以没有用到这种力度比较细的衰减率，是因为当时算法本身和这个kernel优化，他当时都没有优化到一个比较好的状态。所以当时就是考虑到这个效率的问题，被迫只能用那个member two那种力度更粗的一个衰减率。对，然后后面的话算法层面和kernel优化层面的话都是有一些很多进步的。然后到今年年初的这个时间点的话，大家就觉得是不是可以重新来研究一下，能不能把这个fan grand decay把它把这个力度比较细的衰减率，把它引回到这个getting data net里面。你们设计完最初的效果怎么样？
发言人  
最初的效果的话，我记得张芸应该是之前是在他在他他应该是先试了一大堆这种混合追的这种混法。然后他最开始是发现混getty data net比其他的要好。然后后面他就是因为他们kimi内部他是有一个叫做scanning neither的一个东西。就是说你在一个规模下面，你的表现好的话，那你就要到下面一个规模去继续scale。对，就有点像通关一样，他有很多很多个关卡，然后你过了一关之后，他可能要到下一关就继续去跟full attention去比。然后最开始的话可能就发现那种hybrid的getty data的话，他可能就是在有一些地方还是不如那种富的那种soft mass attention。对，然后后面的话，他就开始玩了一下，就是那种把那个decay，把它换成这种更加细腻的decay。然后他发现在他的一些实验下面，他就发现他的那个提升还挺大的。
发言人  
避免linear attention和difficult sparse tension。你自己觉得他们的表现更好，他们分别可能适合什么样的任务？像这两个的话，他们其实是不是想解决的问题是一个问题，就是在这种长文本decoding下面如何解决这个效率问题。然后像kimi他走的是这种混合主义的路线。对，其实千万他也走的是这一条，现在主要是在投入这种混合助力的这种路线。然后像第二次的话，他们主要就是喜欢走这个系数作业的这个路线。
发言人  
他们那个key message attention，包括他们之前发的那个native sparse attention，都是这种走稀疏的路线。然后他们觉得可能稀疏是一种更好的方式，来降低这种decoding的cost。像deep sickness巴萨泰坦的话，他应该是没有full attention的对，所以他应该是每一层都是difficult attention。对，但是他每一层的话他都要把所有的KV cache把它全部存下来。对，然后他只能就是从一个track point，来经过一些journey，然后得到他的那个叫做index的一个东西，来选那些top一个token。
发言人  
所以这个deep sick attention像混合作业的这条路线的话，它还是有一些全局注意力的。然后他的那些比较快速的那些层是一些线性注意力层。然后这个好处的话就是说它可以省很多的KV cache。对嗯然后混合作业的话，它就是不仅它能减少那个KV cache，它能减少很多KV cache。因为它绝大多数层都是这种类似于RN的这种层，然后他同时也能提高这个抵扣的效率。因为他减少了KV cache的size，所以他做的时候可能就可以去用一些更大的这个batch size。因为之前可能放不下，然后现在KV开始被减少了很多。然后这个时候可能就是可以加到这个batch的这个size。
发言人  
像TPCS fast attention的话，它是没有减少KV cache的作用的对，但是他可以通过这个sparse的激活来减少每一个token生成的花费。对还有一家mini max，他们最近也做了一个算法的选择对吧？对，像mini max的话他他上一版是那个linear attention。对他应该算是这种混合线性和平方唔助力的一个先驱。因为他年初发的那个m one的那个版本的话，是一个非常大规模的一个混合助力的一个实践。对嗯然后他们前几天发了一个叫做M2的一个模型，然后这个模型它现在就变成了一个full attention。对他既不是这种混合注意力，他也不用这个sparse attention，他就干脆把它退回了那个for attention。
发言人  
对，这是为什么？我觉得是我觉得他们的这个负责团队他们非常的open，然后他们是分享了很多这种经验，我觉得这些经验都是很宝贵的。比方说我记得他们说他们第一版的，他们第一版的话，他们监控的一些指标，就是发现他们要那个lightning attention的那个模块，在这些指标上面表现的都很好。然后这个learning attention它又效率更高一点，所以他们最后就上这个learning attention了。对，然后后面他们发现如果他们在一些比方说那种叫做multi hop reasoning，就是多跳的这个reasoning上面这种task的话，他发现这个吊点会非常的大对。
发言人  
然后这个的话就当初用那个方案的话，就是因为他们最开始没有去检测这种多跳推理的这个能力。然后他们只主要只看到一些比方说MMLU之类的这种能力，然后他们就选了一个非常就我来说的话，我觉得那个lightning attention的话，它其实是一个比较弱的一个线性注意力。因为他那个机制就感觉最近两年先行出力这个领域发展了很多。然后他用到的那个in the attention，给人的感觉就像是两年前的一个linear attention，就那个技术的还停留在两年前。对，可能很有可能就是因为他们第一版他们的做评价的那一个pipeline不够详尽。
发言人  
然后他们就选了这么一套比较比较略显哪一步的一个方案。然后最近的话他们可能是想做这种agency task，然后想做这种coding。然后像多跳推理这一个能力的话，就是会在这种场景下面变得非常重要。然后他们就发现any attention可能跟full attention他之间的performance的差距还是挺大的。然后他们就暂时退回了全部都是so much attention的full attention的架构。对，但是他们说他们还在继续探索这种混合处理的架构，说不定他们下一版M3又又变成混合注意力架构。
发言人  
你怎么看待大家在这种算法上的不同的选择，或者是反复像历史的话就会螺旋上升。就是一套技术方案就肯定是要经过很多很多验证才能最后定下来的。对，像像m one可能当时就是没有验证比较充分，所以当时就比较草率的上。然后后面发现他在这种对跳推理上面他效果不好，然后就暂时退回来，这个也是很正常的吗？
发言人  
对，硅谷公司现在对于混合注意力机制，他们的探索方向是什么样的？各家公司什么不一样？这个我感觉不能讲了。Open eye什么的可以讲了。Open I的话我只能讲有一些有一些有paper的一些方案，就是没有paper的方案我是不会讲的。
发言人  
Open的话它是它是比方说像GPT3的话，它在它的那个technical report里面就讲了，他会用到一个混合的一个全局的一个注意力和一个local的一个sliding window attention这么一个混合的一个方案。对，像这个的话，它是在GBD3的那个报告里面就已经明确的写出来了，所以这个是可以讲的对。然后像他们最近的OSS的发出来的那一个开源模型，他们也是用到了这种滑动助力的这一套方案。所以他们应该就是一直在用这一套滑动注意力的方案。
发言人  
对，我们待会就讲一下。因为你刚才说linear tension这两年发展也有很多，你能给大家讲一下他的这个发展线索。好。像利尿探险的话，他最开始的话我觉得就是非常的不work。对他就算他在短文本下面他也不work。然后因为他最早的那个attention是2020年发明的。
发言人  
然后我觉得他可能这中间的这几年，就是在language modeling语言建模上面他都没有效果，没有跑到很好。然后后面一个比较有代表性的工作就是right lab，然后他就是通过加一个遗忘衰减的一个机制，然后就发现利尿tension他scale上去，他在原建模上面还是可以取得一个比较好的一个效果的对，然后ryan的话它就是往它加了一个输入无关的一个DK。输入无关的DK的话，就是说他那个遗忘率他是跟输入没有关系的。比方说他的遗忘率是0.99，那样子的话他每过一个token的话，他前面的那个hidden state他就要乘上0.99。这样的话他就遗忘掉他1%的这个东西了。然后他再把新的这个东西把它写进去，这就是一个叫做输入无关的一个衰减，这就是外卖他用到的一个技术。然后这种输入无关的这种遗忘的话，他在之后应该是被逐渐替换成了那种输入相关的一个衰减。
发言人  
就比方说像我之前的一个叫getting linear attention，前面也提到了，就是加了一个门控的一个机制。然后像mamba和mamba two的话，他们其实也是跟注意力的话是很有有很多联系的，尤其是mama two。Mama two的话它基本上就可以看成是线性注意力，然后他加了一个衰减，但是这个衰减跟relate非常像，然后它跟relay的区别就是那个衰减它是由输入来决定的，就是每一个token他的衰减率他就可能不一样。
发言人  
对，就比方说他遇到有一些token，他他觉得这些前面的内容没有必要忘，他就把可以把那个衰减率设为一。这样子的话，前面就根本不去做这种衰减。然后如果他遇到一些token，让他觉得前面这些信息已经没有用了。那那这样子的话他可以在那个位置上用一个比方说让它的衰减就等于DK等于0。这样的话前面的那个state就是被完全的把它忘掉了。对，因为它乘了一个零上去，所以它前面的state就完了，完全没有了。对，像这种输入相关的这种decay，它就是比较灵活，能够通过这个数据来动态的学什么时候该去遗忘，然后什么时候该去记这前面这个state。
发言人  
对，然后这是第一个比较大的改进，就是把这个衰减从输入无关变成输入相关。然后第二个改进的话，就是德塔纳这一走路线，就是把他的更新的那个公式，从最开始那个简单的那个相机量碳水，它用到的其实应该叫做一个叫做heavier rule。对，这个弱的话他就是简单的把key和value他们的外籍，把它加到那个hidden state上面对他这个就是一个黑边的一个如然后像德塔奈这一套模型，它用到的是一个叫做迪亚塔肉的一个东西。
发言人  
第二套入的东西的话就是说每一步的时候，他先用这个key去取出那个memory里面会返回一个值，这就是这个keep。他在这个memory里面他本来对应的这个value，我们管它叫做old value。然后这个key它又会有一个输入的一个value，我们把它叫做input value。对然后嗯因为它是一个有一个关联记忆网络的一个视角，然后每一个key我们想让它只对应一个value，然后模型也不知道它应该是对应前面的value还是输入的value。
发言人  
这样的话我们又有一个可以学习的一个turn，叫做beta。Beta我们可以看成它是一个值在0到1之间的一个一个系数，用来决定我们要用多少的前面的这个old value，然后要用多少输入的value。对，我们会做一个线性组合。通过这个系数我们会来做一个线性组合，然后得到他最后的那个新的value，然后把这个旧的value和key它的外籍把它从memory里面减去，然后把这个新的value和key它的外籍把它加到这个关联网络里面。对，这就是data net，它那个那个更新公式的一个high level的一个idea。
发言人  
然后相比于力量tension的话，它其实是有一个减法的一个操作在里面的。对它不但可以就加法的话可以把它想象成是往这个记忆网络里面去把它去记东西。那么减法的话就可以把它理解成从这个记忆网络里面把它删除一些东西。对，像这个的话就会比较更加有针对性的来删东西。像之前那个decay的话，可能就是很多维度一起在做decay。像现在的话就是只取某一个分量，然后他有一些非常有有目标性的这种山东西的这个操作在里面。对，所以这点塔入围代表的第二个改进的话，就应该是你也要tension这个领域里面最近的第二个改进了。就包括像delta net get data night，像raku seven，他们都用到了这个delta rule。
发言人  
为什么linear tension从一开始效果不好到慢慢的一步步改进，大家相信他还是promise呢？我觉得每一次大家关心你的attention，那肯定是因为大家碰到了一些context war。对，像大家最开始的时候去研究linear attention，就是比方说在2020年左右的时候，大家去研究linear tension。是因为那个时候大家遇到第一个context war，就是遇到一个context这种强，然后他就是撞到这个抢了。他如果想继续提高context，那就只能找一些复杂度小于soft max小于平方的一些东西来了。
发言人  
对，因为当时像bert在那个年代，他的那个训练其实就是512。对，然后当时可能觉得2048当时在当时的那个视角，可能8192就是一个算一个长文本了。因为那个地方就非常的慢。然后后面就随着flash attention这个技术的诞生，然后就打破了这一堵墙。对，然后现在看来就是8192，这是一个非常短的一个文本了？
发言人  
就是在这上面做训练是没有一点压力的。但是在之前就之前的话他没有flash attention的时候，他是计算的话，他需要把这个平方的这个attention矩阵要把它先把它要把它实实例化在global memory上面。然后要把它把它从global memory里面把它扳回flash memory里面。这样的话他那个memory的读写，他整体的开销是非常的大的。
发言人  
然后同时因为这个attention矩阵会被实例化，在那个global memory它可能就是会带来一个out of memory的一个问题。这就是最开始大家研究力量tension这个motivation。然后随着flash attention的话，大家就发现唔这一堵墙其实已经破了。就是这样我们能用这种excel的方式来直接算这个soft mysql attention，那我们就没必要找一些力量的去逼近他。对，所以大家音量太大的眼睛就开始没有那么受关注了。
发言人  
然后直到最近，就比方说像这种常温班的decoding，又重新成了一个需求量非常大的一个东西。然后就是这种23个model，它要涂很多很多的token，然后要做这种decoding，然后这个花销的话就会让人们又不由自主的又重新来审视这一套的技术。然后这一套技术它本身在学界就能又有了这么久的发展，尤其是在flash attention之后。
发言人  
学界其实也意识到，如果像利尿tension这一套模型，如果想让大家被大家接受的话，那它硬件上面的效率是非常关键的对这也就是为什么我最开始的时候就是搞一个叫做flash linear tension的一个project。就是致力于把这些你俩太深的这一些变种，然后用threaten把它在写了一个库，然后写了很多课。让他能够在当代的硬件上面，主要是GPU上面能够来快速运行。所以它核心是效率更高，价格更低。就是每当soft mice它的它的效率变成一个平行的时候，大家就会回来看linear attention，大概就是这样子的一个历史mini attention。
发言人  
现在是工时了吗？我觉得力量tension，我觉得现在的共识是纯粒料tension是不work的对，就是他在这种长文本下面，它是有一些比较fundamental的一些缺陷的对，然后现在大家一般都不会去尝试这种纯线性的这种对嗯嗯这种模型。然后像一些比较折中的一些方案，像这种混合主义的话，他还是有很多很多的线性注意力层。然后他但是他还是有一定数目的这种全局注意力层。这样子的话它这个模型它的下限是有保证的，对它处理长文本它也是有一定的保证的。因为它终归它还是有很多这种全居住地层。像权限性的这个网络的话，他可能从理论上面他就没有办法做那种长文本的task。因为他的那个RN的状态数目是恒定的。
发言人  
然后随着那个contest它的那个长度增加的话，那他早晚会存不下，那早晚会唔会损失很多那种精度在里面。但是像混合追的话，它有很多全局注意力在里面，所以还是可以通过这些全球注意力来完成这些长文本的task的对，然后像比方说像kimi linear这个paper，然后像之前千万三那个困three uh next。他就是他们那个常温不比如说像ruler，比方说像其他task那个表现没有掉点对，所以他在查文本上面还是有一些有一定的能力的对，然后混合作业就会受到很多很多地方的关注。但是我也不知道他算不算共识。对，因为不同的地方还是在尝试不同的方案。比方说现在比方说像deep sea，他就在尝试这种sparse attention的这种方案。
发言人  
在拼命的这个论文里面，你们提出的是每三层的KDA，也就是kimi delta tension增量注意力机制，插入一层全注意力机制，full attention. 这个比例是怎么确定的？这个比例重要吗？我觉得3比1现在也快变成一个工时了。对，像minimize它之前是一个7比1的1个比例。对，然后7B1的话可能soft mass attention那个层数不够，然后插文本的那个保证可能没有那么好。
发言人  
我记得之前自己他也发了一个paper就是来研究这个hybrid架构，他需要百分之多少的so much attention。然后他们的结论也是说，他们做了很多pre train from scratch的一些实验。然后他们就是通过改那个不同的力量tension的这个module，然后他也改那个混合的比例。然后他们的结论大概就是说3比1的这个比例是最好的。然后galaxy这个模块就是比其他的那些另外的那些看着的要好。对，所以3比1，然后后面千万3 next的话，他也是用到3比1，然后换galaxy点那个访问。然后这个方案应该是不同的厂商，他们探索出来都是觉得这个比例是更好的。
发言人  
对，然后这个可能是最开始minimize没有验证充分，之前也说他们可能最开始的评测还是有有一些不足，所以他们用到了一个更加aggressive的一个方案，就是7比。然后现在的话基本上就是都回到3比1这个上面来了。然后我觉得3比1应该就是一个在这个不共识的这个hybrid linear里面的一个共识了。就是大家用3比1的1个比例来混这个模型。
发言人  
是不是你们在算法设计的时候始终要平衡表达能力和计算效率，这两者是它的核心。比如说北极星指标，确实我觉得还是有一些trade off的对，像全局助力的话，他如果太少了话，我感觉就比方说像那种reason reasoning task，然后相差文本task，它肯定会会受影响比较大。他可能一些short contest的一些task没有什么影响，比方说MMLU，但是那些查文本和推理的task他应该是能看到的，那个现象会比较大的对。
发言人  
但是从另外一方面来说，也不是说attention层越多越好。因为像大家如果训完之后会发现绝大多数attention层他可能就是没有用的。对。然后他只有一些关键的层的attention，它是有用的，它不是每一层的tension都有用的。然后这个网络它本身自己它就是有一个冗余都在里面的这样子他就给我们带来一些机会，就比方说我们可以把一些层把它换成一些线性层。对，所以混合的架构就不一定，我觉得他不一定代表他比那个global要差不。然后他有很有可能就是说他可能是一个全面更好的一个替代方案。
发言人  
然后像mini max他之前他们也发了，他说他承认了一点，我觉得非常的好。就是说他们发现heavy力量tension或者hybrid这种滑窗注意力在长文本的multi hop多跳推理上面会有缺陷。对，像这个的话，就是我觉得这个应该是现在hybrid它唯一的一个问题。对，因为它就是就我就我所知的话，我觉得我就知道他在其他task上面，基本上是不会比去，全部都是supermassive tension要差的。他只会在这个，这个也比较好理解，就像这种多跳推理的话，它就是比较吃这种token和token之间的关系。所以他可能就比较吃他这个software attention他的这个层数。
发言人  
对，这个我觉得非常吃这个全局注意的层数的。这个任务不是很多，可能就只有这种多跳推理，然后这种查文本做reasoning，这种会稍微迟一点。然后其他很多task基本上不吃的话，那他是完全不会受影响的。然后像这种多跳推理这个task的话。我觉得如果我们去开发一些硬件高效，但是他表达能力更好的一些RN的话，他这个gap他是有可能被直接被缩小，甚至会反超这个gap的对，比方说像kimi他最近这个linear，张宇之前玩就发现就是把这个力度粗的这个DK换成力度细的。
发言人  
Decay之后，他在这些multiple reasoning coding和math这些task上面，它提升还是比较可观的对，然后就是说这些task就是hybrid可以做的更好。然后现在我觉得混合线性注意力只是一个开始。对，然后我觉得整体的还是很有可能做出更好的这种混合租赁机制的。
发言人  
就是可以去叼一下那个线性作业的那个模块，你在过程中有给kimi什么算法建议？对我张宇想玩那个翻过来DK，然后我就帮他想了一个创壳那个并行的算法。对，感觉这个应该可能就是我对这个工作的唯一的贡献了。对，因为他这个都是基本上都是张宇在kimi做了很多很多application study，基本上都他做的。对，所以credit基本都在他那里，不在我这里。
发言人  
然后像这个算法的话，我觉得其实也是之前有一篇文章叫做combat。然后他就是涉及到一个新的算法，能够把那个给他那个他那个求逆的那个算法把它减少一次。对，然后我看完那个算法之后我就发现，我就可以把那个GDN叫get gonna他把它减少一次。然后我又紧接着我又推了一个能够适用于KDA的这个算法，然后我就把这个算法告诉张宇了，然后张宇他就去写课，我去实现这个算法，然后就发现这个算法还是它的这个scalability来说，还是比之前的那个算法要好一点的。
发言人  
对问一个很general的问题，是一个研究员想问你的attention到底应该怎么设计？这个问题的话我觉得现在可能就只有两条路，两条比较主流的路线，一种就是hybrid线性，然后一种就sparse。对，然后我觉得这两种它其实都是非常的promising的。
发言人  
另外可能有一些比较非主流的一些attention设计。就比方说我上我看上次meta还放了一个论文，就是搞一个3次方的attention就是显平方复杂度他还不够，他还要搞一个3次方的。然后像有些地方它有一些比较有意思的一些平方复杂度的一些attention的变种。比方说by dance他之前有一个叫做delta formal的，就相当于把delta rule的思想把它引入到so much，能够让他表达能力更强。然后像这个工作我觉得也非常有意思。然后改进作业的话，他要么就是把收把supermarkets让他做的更好，要不然的话就是做一些更加高效的一些wearing，比方说sparse attention或者这种混合线性的这种attention，然后这两种我觉得他也是可以结合的。
发言人  
对，他们有各自的优点和各自的缺点吗？像sparse attention的话，它还是做retrieval肯定要更强一点。但是它的缺点就是说它KV开始它不能省，然后像线性的话，它可以省很多KV cache。所以我之前写了一个知乎的回答我就说这两种方案为什么我们不能把它结合到一起呢？
发言人  
就比方说我们可以让sparse attention去取代这种混合注意力里面的全局的注意力。这样的话我们就不需要有一个全局助力的复杂度在了。但我们还是要从那个KV开始，但剩下很多层的KV开始，就可以通过这个线性注入力把这个KV cache的size把它打打下来。这样子的话，我觉得可能就是我目前心中比较理想的一个高效的一个价格了。对，就是因为他们是不高效不掉点而言的话，是这样子的。所以leading attention和sparse attention的未来的关系可能是融合到一个统一的框架里面对吧？
发言人  
对，因为我觉得先行attention和sparse attention它其实没有什么竞争关系。我觉得线性attention的这个竞争对手可能更多的是sliding window attention。对，像比方说像GPT3，他那个论文里面提到的那个全局混sliding window。那sliding window的话它如果让这个线性去取代这个sliding window，能够让它更好的话，那和未尝不可能对吧？所以你觉得怎么把linear tension和sparse attention能够做更好的结合？现在有人在探索这件事吗？工业界的话我觉得我还据我所知，我应该没有看到有人在同时去结合sparse attention和线性attention。但学界有一些工作还是有一些这方面的探索的对，就是有些层用sparse，有些层用linear attention。
发言人  
所以deep six上选了slice attention，keeping选了linear attention。这其实可能也是阶段性的对吧？他可能未来会会就是大家会探索一条新的路，就是把两者都结合，现在也不是face级别的关系。
发言人  
对我觉得混合作业的话，它decoding长度上去之后，他的问题就是说他还是会被这个全局注意力，他的这个效率把它放得住。对，然后后面那个瓶颈就主要在这个全局出去的那个效率上面了。这是对，然后像全部都用那种sparse attention的话，它瓶颈可能是在KV cache的管理上面对，因为他还是不审KV开始。对，所以的长度上去了，就可能要做很多各种各样的KB开始压缩的工作。对，然后两者都是有还还是有各自的问题的。他的结合是比如说可能是不同的层用不同的attention？我觉得最好的金额的话就是把混合注意力，它里面的全局的注意力把它换成sparse attention。我觉得理论上只要sparse attention他能选得准的话，他是完全可以取代唔full attention这个层的对，但他现在问题可能是选不准，对，这是一个很大的问题。
发言人  
对，然后这也是为什么可能是为什么就是difficult他第一次放的那个DSA，他要用蒸馏的方式来尽可能的让他那个index来选token选的准一点。对，这也有可能是一个原因。Ok选得准选不准的核心的瓶颈在哪儿？我觉得就是学习难度。对，像sparse attention的话，如果你从头开始训练，他可能那个T都不太准。然后他可能觉得他就选不准那个block了，他他学会选block还是挺难的，还有各种那种稀疏梯度的问题。对，像sparse的话它经常就会有这种问题。然后像针灸的方式的话，它其实就是已经让一个训好的一个全部都是so much attention的一个teacher model，来来针对一个他那个token的选法，这个已经就可以选的非常的好，这个从学上面来说也是make sense的。
发言人  
你觉得kimi这个工作它相比年初的MINIMAN的工作，它的进步在哪里？它主要就是在于线性助力，它那个模块他还是会好很多的对，就像我之前说，就是lightning attention给人的感觉就像一个两年前的工作。对，就还挺有在retina的那个版本。
发言人  
对，然后像这两年的话，我觉得现金注意力还是有很多发展的对，然后这些发展我觉得都是work的对，就是已经有其就比方说千万和kimi都发现这两年有一些进步。比方说那个门控，比方说那个貂塔，如果都是有用的。对，嗯所以把这些最新的进展把它融合进来肯定是更好的。像kimi甚至在之前的工作的基础上面，还把新开发了一个KDA。对，然后让他的那个模型的能力会更强吗？对嗯然后另外的话可能他还有一些其他不同，就比方说MO1的话，像kimi他应该用的是翻归MOE。然后m one我记得他那个MOE好像还比较粗，他还没有用到这么番贵的这个MOE。对，所以就是有很多种可能性。
发言人  
如何做一个公平的比较？比较一下linear attention和sliding window attention，seminal attention和你2 tension做公平的比较的话，我觉得可以有两种。一种的话就比方说控制它的这个state size，就是sliding window的话，它要开微开始，他要KV开始的话，这个KV开始他是因为他是滑窗，所以他他那个KD cache的上限是被bound住的。这样的话我们就可以把它这个KV cache的他的这个size当成settlement attention了。他一个stay size，然后RN的话它有RN的那个state size，对它有那个状态是然后如果这两个东西大概在一个level的话，我觉得就是一个公平的比较。
发言人  
对，因为像decoding的时候，他sliding window和IN因为decode的话，它基本上都是一个memory bond的一个过程。所以只要他的这个state size差不多，那他decoding的效率就不会差太多了。因为memory棒的话它主要就是看它读多少state。然后只要他们这个state差不多大，那他们这个抵扣点的效率基本上就会差不多大。因为ID coding还是主要是memory bounded的。
发言人  
对我说到算法的就是眼睛，他最早从transformer到MOE，然后到现在大家探索linear tension或者sparse attention。它的这种渐进式的创新，你觉得他优化的最终的目标可能是什么？然后最终可能形成的一个算法的共识会是什么样的？我觉得这些优化基本上都是体现在给定你相同的flop，你怎么去更好的利用这些flop，然后取得更低的损失。函数。
发言人  
对，像MOE这个技术，就是前两年可能比方说2023年的时候都在传GPG4UI贸易，但也有很多地方不太敢跟的。然后像现在的话，MV基本上都是已经变成一个显学了，就是每一家都会做这种犯规的MOE。对，这个ME的话它其实也是一种他可以想象成就是更加高效的一个FFN的一个替代品。对，就是它可以更好的去扩大那个ipad的这个参数量，然后同时他又保证他那个flock普遍这样子的话，他付出相同的flop，他能在预训练里面取得的那个嗯训练的loss就会越低，对吧？这就是一个点。
发言人  
然后我觉得MOE他可能是近几年突破最大的一个，在架构方面突破最大的一个方案。然后下一个突破点可能就在attention，因为transformer就两个模块，一个M一个FFN1个attention。现在FFN基本上已经雕成了这种fan grand MOE的这种形状。对，然后attention我觉得大家也是可以来雕一下的对，why not对？
发言人  
这样的话他比方说在常文本下面的话，他付出相同的floor，他可能去做你的那个loss也会更低吗？对我觉得这两套思路都是要的，就是减少flop。然后能够让它像FFN的话，它减少flop，它就可以去用更大的这个参数量，更大规模的一个模型。对，就比方说你总参数量就可以堆高了，因为你这个FF分的这个算力减少了？大家都知道就是在大规模训练下面，FF分的那个计算是主导的。对，然后把它换成这种犯规M1的话，它其实是能降低很多很多这种cost。
发言人  
对，然后attention他scale的就主要不是参数量，他scale的就是那个context这个window size。然后如果这个attention它的这个flops就在长文本下面，能够把它打下来的话，那我们就是做那种常文本的这种生成。然后比方说你有很多agent，他让他去处理很多很多workflow，然后为很多很多context给他做，这样的话他也会benefit from更大的这个context window的对那如果把模型的架构比作，比如说大脑的结构。你觉得MOE和attention他们分别代表的是大脑的什么组件？能能这样去形象化的去理解吗？像tension的话，它应该就相当于working memory。对，就是那种工作记忆。它就是然后像FFN的话，就有点像那种我忘记人的大脑存那种消息的那种还在发生。
发言人  
对，可能就是海马体就是来存存储这种信息的，就是过去信息的对，这样FFN它基本上会被看成是一个电子队的一个关联网络，它可以记下很多很多这种knowledge。对，就像这种word knowledge都会被他记到这个FI分里面，这就是一个一些word loader就会存下来。然后attention的话就是比方说你在一个新的场景，然后你遇到新的这种喜来登，然后你会读到新的context，然后他会在这个context window里面就动态的来来做处理这些信息。那就有点很像我们人大的那个工作记忆，那个working memory对他更偏及时性一些。
发言人  
对对对，当现在数据遇到数据强，数据的瓶颈比较明显的时候，是不是算法的创新变得更重要了？我觉得是的。对，就是因为你要在邮箱数据里面去压缩更多的智能。对我觉得之前的话，就比方说你数据一直能scale的话，你谈这个data efficiency就是没有什么特别大的用途。因为大家别人已经加这个数据就行了，就让他模型继续scale up，然后继续加数据，所以大家都不需要去动算法了，然后大家就只需要买卡就行了。
发言人  
对，然后现在如果有这种数据抢，然后还有这种三年前的话，那可能就到最终还是要回到这个算法这种本质的东西上面来的。我觉得这些东西都是缺一不可的。就比方说像data像这种算法，像这种算力，就是三就三有点像三匹马车，就是来驱动整个人工智能的发展。对，然后我记得之前像OPI的CTO他也说可能在这个节点上面，算法的研究的重要性可能会被重新抬高。对，如果你记得那个采访的话，他应该是这么说过的。
发言人  
对你觉得现在的架构transformer架构它的天花板是什么？他的天花板，我觉得还是先把efficiency的问题解决掉。对，因为现在还没有解决掉efficiency的问题，可能他处理一个很长的一个context window还是有一些局限性。对，所以大家会做很多上下文工程，做一些RAG，来来通过一些其他的方式来来来做这些问题。
发言人  
但如果你这个context的问题把它解决掉的话，那你RIG这一套技术都不需要了，你直接把它放到context里面做in context RH就行了。对，然后我觉得天花板的话就先看看能不能把全局这个注意力把它干掉。对，这是第一点，就是因为他确实他是阻止这个connect window继续scale up上去的一个主要的瓶颈，所以这个瓶颈我觉得是早晚都要把它把它弄掉的，这个是第一点。然后第二点的话，可能就是continue learning。对，然后像现在这种transformer架构还是没法做continue learning的。对，然后之后continue learning，让AI自己学习这种甚至。甚至大家不都想把pre training这个地方变成直接从IL开始，让让这个模型直接从零开始学，不给他喂这种present data吗？对，像这种新的范式可能就是之后的这种探索。
发言人  
对一个研究员问你如何把linear tension的transformers grow up继续扩展。我觉得scale up我觉得scale up应该是没有什么特别大的问题。然后我觉得可能还有一点的话，那些配套的这种infrastructure，还是需要继续答，像flash linear attention只是提供了一些threaten的一些kernel，基本上就是可以凑合用。但是他的那个效率肯定不是最优的，因为他是摔疼写的。所以如果有志向投入这个领域的，比方说一些公司或者可以花一些精力去优化这些kernel，对这个是对继续scale up上去有好处。
发言人  
然后像influence那边的那种，我觉得现在像infants那边的支持已经在逐渐变多了。可能就比方说像半年前我参加mini max，他他有一个圆桌讨论一样的东西。然后当时主持人是金贤老师，金贤老师问我这个领域它主要的瓶颈是什么？我当时说是英法的那个配套没有跟上。对，然后当时就像老师还觉得挺意外的，因为我会回答一些别的东西。
发言人  
对，然后其实事实上就是这样子，我觉得算法层面可以比如说像金两元的这个发展，就已经可以去大规模的来试了。比方说千万3 next和keeping那个这些都是可以大规模解释的了。然后后面deploy的这种瓶颈的话，可能比如更多的就是在这种配套设施。像因为这两家发这两个模型，那开源社区支持力度也挺大的。像之前比方说SGN它都不支持这种hybrid model做inference。然后现在就是趁着这个机会就可以把千万3 max，然后像minimize m one这些模型，就是加一些这种推理的这种推理引擎的这种support。
发言人  
对我觉得是一个真相的一个流动，就是真相的一个领域，真相发展的一个过程。就像这些做寂寞的厂商，他们去做一些比较promising的结果，然后把它发开源，把把这些开源位它把它发出来。然后那些做推理引擎的人，就会有很多动力来想办法来支持这些东西。
发言人  
当这些info的配套更好的时候，比方说别的公司可能就是觉得像你那样贪吃。他的英法的那个环生态太太差了，可能就算做出来这个生态不好，可能他他实际上deploy他的成本也很高。对，但现在如果只要这个生态做起来了，然后就会有一个正向循环的一个作用。
发言人  
我觉得你觉得现在中国的算法创新相对于硅谷来说是差不多更强还是落后的？我觉得国内算法创新肯定是更强的对，主要是in terms of架构的话，那肯定是国内更强的。我觉得这也是有一些生态地位不同。就比方说国内没有那么多卡，然后他们其实对这个efficiency的要求是更高的。所以他们更有动力来尝试这一些更高效的一些力量太太深这样的变种。然后像硅谷有限公司基本上就是卡太多了，他们就懒得搞。
发言人  
对，反正三驾马车你总得有一辆跑得快一点。对对对，他们有那个算力，那也能凑合跑吗？对，就是脑子长得不怎么样无所谓，反正我先Sally对上去。
发言人  
对，然后我觉得硅谷这边感觉美国的公司会更注重优化一点。对，就像optimization，对，就比方说优化器，对，像国内公司也感觉在逐渐在用，就比方说像kimi他也是最早吃没有这个螃蟹的一个地方。对，然后给我的感觉就是美国他们对优化器的投入明显是比国内对优化器的投入要大一些的对kimi的linear tension的效果跟去年deep sick sparse attention的效果比哪个更强？我觉得效果对比的话，需要有一个地方来做一个apple to apple的一个比较。
发言人  
对，因为这个东西就是非常的tRicky，就是不太好比。我觉得不同的地方训出来不同的，他可能就是完全不能比了。因为他那个训练架构，那个data recipe，那个优化方案，完全都不一样的。他就没有一个apple to apple的一个比较。对，像kim linear他最近这个report，他还有一点就是说他有一个apple to apple的一个跟负二碳是哪个比较对，它是有一个apple to apple的比较的，但它没有apple to apple to fast attention的一个比较。要是有一个地方能做慈善，来apple to apple来比一下，让大家能更好的去知道就更好了。对，但现在因为没有人在做一个apple to apple的比较，所以这个问题我也不知道哪个会更好。
发言人  
对，这个很有意思，为什么kimi不做这个比较？它比较的是for attention。他在看到我们写的时候，他们是第一个验证了性能超越for attention的。混合linear tension架构，可能还是资源有限。
发言人  
对，如果就有那么多卡的话，那可能先投入一个路线去验证吗。对，然后如果验证出来了，再去投入另外一个路线，看看有没有可能。比方说把全局注意力再把它替换掉。
发言人  
对，感觉对，感觉就是没有这么多卡来同时来跑一些不同的方案的对比。然后像硅谷的话就有很多东西都都闭园，所以你也不知道他们有没有跑一些apple to apple的比较吗？你看那个kimi linen的论文，你觉得还有哪些是指大家关注的？前面说就是这个线性注意的模块。
发言人  
对，然后还有可能就全集注意力，他的那个用rope还是用nope的一个比较。对，像kimi他选的是用nope，然后像千万3 nice的话，他是选的是一个partial rope。对他就是25%是rope，75%是nope。对我觉得在这种混合主语里面，大家都在砍rope。但是看大家砍多少，像千万三大家看了75%，然后像kimi砍了百分之百。
发言人  
对，然后像这种长度外推，这种感觉现在看起来的话就是rope在这种在这种hybrid架构里面可能会比较阻碍这种强度外推。对，然后这个地方其实也没有共识，大家也不知道是用有有些还是用rope，有些还是用note，我觉得这个地方还是没有共识的，然后有些地方还用partial rope。对，第一个就是题外话，你有关注最近deep sick他的那个新的工作，就是TC他发了一个OCR的paper，就知道大家其实数据撞墙的时候，还是有很多那种书籍，PDF里面有大量的数据的。所以他们做这个OCR可以帮他们更好的洗一些data出来，然后来做pre train。然后另外的话，他们说是用OCR来做这种context compression。这一点的话我觉得是一个有意思的一个劳动。
发言人  
对，但我不确定这个方案怎么样。对，千万的工作你有参与没有？就千万3S的话，我就基本上类似，我会他们要是碰到什么问题，我就可以帮忙打一下。对，就是不参与他们去模型什么的。如果他们有一些学术上的讨论的话，我是会跟他们讨论的。对我跟千万3 nice训练的那几个同学都还挺熟的对，那个minimum参与没有，ok所以你参与的时候参与了他们应该不会用这个访问我会觉得这个方案在开倒车ok。
发言人  
我觉得你用词很好玩，你总是在用把这个架构玩一下或者刁一下这种词。这个是一种研究员之间的文化吗？嗯，我觉得雕这个字好像还挺常见的，就是有种雕花的那种自嘲的那种说法？要是丽姐说比较时尚，貂华是吧？
发言人  
现在没办法，算力不够，数据也有限了，所以只能雕。但是对但是我觉得第二架构还是挺有用的，像DPCKMOE那个标出来之后，大家都已经成为一个共识了，基本上等于一个共识了。就很多地方会用那个DPC跟那个ME的方案吗？对，如果在他之前，然后他他在做那个，那可能大家也会说可能在调MOE。对，然后感觉雕已经变成一个常见的形容词了，形容我们觉得它不是一个贬义词了。我觉得它是一个把一个模块把它把打磨到更好。
发言人  
对，如果数据的screen能够非常突出的话，其实没有必要调就怼数据就好了当数据还很少的时候，他比如说机器人领域现在就是数据就没什么数据，那只要加数据它就能够显著的效果提升，那就没有必要去做模型进行算法的创新，对，这是一点。所以robotics他最最应该做的应该还是先把数据这个问题把它搞定。对，然后数据搞定之后，再回来看这种efficiency的这种问题也不迟。
发言人  
对，你做AI的researching心里的是什么？好，真正你是怎么进入AI这个行业的？原来的行业的话就是本科的时候就对machine learning、deep learning挺有兴趣的。然后当时master在商科大念那个NLP，那个时候就已经进入AI了。然后22年就是Cherry GPT这一波，就是large language model分离开来，做LP的人基本上都来做那三个是model了。
发言人  
对，然后做AI我觉得现在更有意思一点，就是比之前会更有意思一点。因为之前可能大家还是在分task来做，现在可能都比较unified，可能就是会比较focus on更加通用的一些问题来了。对就不需要去操心某些特定的task，因为你只要穿一个很好的基膜，你对不同的task都可以用。你无非是postcard的时候，你要注意的地方不一样。然后现在的话感觉自己做的东西的话，可能就是能看到更多的影响力。然后这一点感觉还是挺看到自己的开发的东西被大家用还是挺开心的。
发言人  
你过程中有遇到过什么样的挫折？没有我感觉我读PHD好像这些工作都还挺顺的对吧？没啥是吧？对，然后感觉就这些工作都还挺连贯的。对，然后挺顺的。我觉得还是因为可能是图片啲之前就是花了半年的时间来调研这些东西，然后可能对这些这个领域的理解会深很多，然后就是跟这个领域来做，其实问题也不是很多。因为对这个领域非常熟，然后碰到什么问题大概也知道怎么去解。
发言人  
对图片是提前花半年去调研，这个是做的是你什么样的工作，是入学之前的半年是吧？之前申请完半年，就申请完之后有半年可以自由的时光。然后当时就基本上就是在调研这种架构的这种paper，然后当时读了很多比较老的paper，就比方说像delta net，它最早是2021年，就是那个LSTM支付的那个paper。然后当时我就对这个工作有印象，然后后面的话就后面就那年年底就做完那个。Getting linear attention. 
发言人  
然后发现这个领域的话，大家会对那个encounters recall，就是从前面的文章里面去做一个retrieve这个task会感兴趣。然后这个就让我一下子联想到了2021年的一篇工作了。对，因为之前的整个领域他把握的非常的通畅，所以我知道如果你们大家其他人关心这个问题的话，我应该从什么角度去切入。
发言人  
然后我也知道他前面工作有什么问题。比方说2021年data net的话，它是没有hardware efficiency的一个保证。对，然后我就觉得然后我后面就觉得就交易了这个工作之后，做data night的话，我就知道data night是一个很好的一个模型。它的缺点就是现在大家还不能大规模用起来。如果我能开发出一个算法能把它scale up，那就是一个非常有意义的工作。对，然后我大概就是这套逻辑链，就可能也是运气好吧，就三号正好能推出一个能够把它scale up的一个算法。
发言人  
对，然后后面的话后面的话就可能就是像KG呆呆的话，就是沿着这个工作做。因为当时发现，但是他还是在很多task上面是打不过这个member to的对，然后我当时就是觉得打不过就加入，那我就把妈妈Q的他的那个getting把它拿过来，然后把他他说再加回来。对，这样子的话就把它A加B变成一个get data net。
发言人  
对我觉得我感觉我做的东西就会看这个领域他需要什么样的工作，然后哪些做什么样的东西会带来更多的这种领域的影响力，然后还有业界的影响力。对，然后如果当你很清楚你要做什么的时候，你其实是不会遇到什么挫折的对，就是技术technical的那种chAllenge，我觉得都是有办法把它搞定的。对我觉得更大的就是你不知道你要做什么东西，你不知道做什么东西是有用的，我觉得这个才是最大的差距。
发言人  
因为你核心是从历史中学习了很多，对吧？我觉得我还是挺喜欢看最早的那些paper的，我觉得那些paper写的都挺好的，然后我管这个叫做考古。对，因为我就喜欢考了一些古代的paper。古代的话可能2021年也算古代。因为就能现在一年前的paper叫老paper，那五年前的paper那肯定叫做古代的paper了。
发言人  
对，那半年你读的最老的paper到什么时候？最近半年吗？就是你调研的那半年那半年可能就是读到比方说二零一几年的文章，读了多少页。我觉得我可以说这个里面的文章我基本上都读过一遍，对不对？这个做到的人很少是吧？对我觉得不同的人有不同的research philosophy。我觉得就一定要把这个领域里面值得看的文章全部都看一遍。
发言人  
对嗯你为什么在AI的众多领域分支里面，你喜欢的是架构？因为我比较喜欢做算法。然后当时觉得nash model就可以看出哪些东西是值得做的。对，然后就想做一些比较通用的，整体都是对这种拉伸任务的有用的一些work。然后结合一下自己兴趣，发现还正如最开始的时候说到，就是像heavy research他们有很多博客，主要还是自己喜欢做算法，然后就发现这个领域很适合我。
发言人  
你数学是不是很好？应该还挺好的。对为什么音频这篇论文里面这么多公式？我觉得这些数学倒不是很难？都是一些矩阵的一些主矩阵的一些惩罚之类的东西。然后像线性注意它URN的形式的话，他会有一些recurrence，会有一些线性转移方程那些公式。然后并行的话可能就是并行确实数据会比较多一点。
发言人  
对，那个东西比较chicken。对他这个论文里面显著比其他论文的数学公式是要更多的。因为我觉得先信注意力，它主要就是一个玩矩阵变换的一个东西，它还可以把一个平方的东西变成一个线性的，然后他又他就是玩这种矩阵变换。对，然后他从recurrent把它变成trunk，他们都等价的，但他们都涉及到很多这种矩阵变化吗？对，所以他数学多一点，我觉得很正常。
发言人  
你刚才提到你读博士前半年做了很多算法的考古。给大家讲讲算法是怎么一步步引进到今天的这段算法历史。我从transformer开始讲，就上次放的话，他感觉可能就三个主要模块。它一个就是注意力极值，然后另外一个微这边，最后就是FFN。
发言人  
最开始那几年我感觉可能加个research非常多，然后有一些架构的改进也确实被用到了今天。比方说像相对位置编码，比方说像rope，它最开始的话transformer的话它是绝对位置编码，然后像今天基本上都改成这种相对位置编码了。然后像MOE的话，我觉得可能也是从2021年左右就开始发展了。然后可能中间有一段时间大家可能就不怎么信贸易，然后后面又发现比方说像DPC1个白毛衣做通了，然后大家又回来重新做MOE，然后现在MOE应该就是他都会用的东西。
发言人  
然后像attention的话，attention的这种变种可能就更多了。像前面我也说到7 2020年前后，可能attention的变动就非常多。其实也主要就是两种变动，第一种就是线性注意力，然后第二种的话就是吸收注意力。他们信息数据的话，他们就会搞很多那种kernel method to approximate service attention。然后在今天来看，我觉得这是一个非常错误的方向。我觉得就不应该去用kernel method去去估计这一些surface attention。然后有一些好工作的话，可能就会因为没有follow up，然后被埋没在。文献海里面，比方说像碉塔恩纳这个工作，我前面也说他是2021年就有了，然后对他入那个东西，可能后面几年就根本没有人take it seriously，就是没有什么full of work。
发言人  
然后从时间的眼界上来说，演化来说，可能有一些技术。比方说像这种细力度的这种遗忘，可能很多年前就有了。比方说像这个细力度的这种DK，至少ERMRP2022年他可能就有一篇工作了，然后最早的话我可以考古到2016年，但后面的话比方说像runner 2023年，他反而用的是一个遗忘速率跟粗力度的一个DK，所以我觉得可能就是之前的技术书不能更好的传承下来。然后我又比较喜欢把所有之前所有的技术全部重新审视一遍，然后选一些我觉得这makes sense的技术来做。然后可能就比方说像德塔路这个技术又可以重现光芒。对，但如果如果没有我来follow的话，可能就不好说了。可能就是可能这一套技术路线可能就会隐藏在文献海里面。对。
发言人  
然后像fast attention的话，他们最早可能就做一些static的一些sparse attention，像Normal formal那种big bird，然后他们会有各种各样的sparse attention，然后好像后面就逐渐收敛到用sliding window了。然后可能近几年的话，他会有一些不一样的东西出来。就是早几年比较少，但是资金又比较多的。比方说像动态习俗，像keeping the mobile，然后deep sick的这种sparse attention都属于动态系数。对，然后总的来说就是我感觉整体还是上海还是在不断演进。然后可能他整个发展就是需要有一些技术可能需要rethink几次。
发言人  
对，然后很多多少少的话感觉这个发展还是会有一些有点螺旋上升的味道在里面。其实历史中已经有很多的工具，但是今天我们需要拿哪些工具来运用，推动今天的算法引进，其实是一个很关键的事儿。对的，对我觉得其实很多历史的算法，他其实很先进的。但是可能但是同行没有意识到这个工作的价值，对，然后可能那个工作就被埋没了。然后也有可能就是那个工作它的配套，比方说那些代码开源代码做的太难了，然后其他人想follow也没法follow吗？对所以对，所以总的来说就是我觉得如果今天做工作的话，可能的话就是不用上网，我就会把这种代码把它做的好，让大家好用。所以这一套技术肯定能把它让它流传下去。对，然后别人的工作之前的话，我就会我会找一些我觉得make sense的一些工作，然后让他让他尽可能的看看他这个潜力有多大。
发言人  
对，然后又说回就比方说像架构里面的算法的话，因为他算法的这种variant也太多了。然后是架构的话，那肯定还是需要很多算力来试的。然后有很多算法可能只在小规模下面有work，然后可能到大规模就不过可能这个是非常常见的对好可能对可能比方说像今年国内公司又对开源重新有兴趣的人，可能今年可能会大家见到价格的这种开源工作会更多一点。
发言人  
然后变化可能会相较去年来说，可能大家会觉得价格的变化比去年会多很多。你的delta rule是什么给你带来的灵感，就是2021年，那个工作就你的工作是他们提出来的。对，然后我就想了一个并行算法，其实就是挺类似于flash attention，指与soft mass attention。对，其实就是一个算法，能够让他硬件靠效果来实现的。如果没有flash attention，那software attention也走不到今天。然后像没有那个并行算法的话，那得他那肯定也不能走到今天的。就大概是一个这样子的关系。
发言人  
然后我觉得我做research可能就是比较喜欢从实际上的这种硬件的这种亲和力来研究的。就是看因为我看一个算法有没有潜力，我会来分析这个算法它的这个嗯并行潜力有多大，然后它的scalability会有多大。对，然后我会在历史的文献函里面找出一些machine learning上面make sense，同时我又能想办法把它并行的一些算法来玩。对，这是我的做research的思路。我觉得总的来说还是我们现在进行上面make sense，然后他这个算法他又可以有变形的算法，就是这样的算法才能被在在这个年代被用到。因为you know就在那三个是model的年代，就是scalability左右你的，你肯定需要有一些能够scalable的算法。
发言人  
对，然后如果一个算法它就跟make sense，比方说像德塔路这个算法，我觉得他们这个算法就非常的make sense，然后同时又能scalability就比较好的话，那就完全有可能就是在今天这个时代上面是会有带来一些不一样的一些架构。就比方说像前文3 next和kimi linux就已经让我们带来了一些新气象。对，就是这个新架构领域。
发言人  
我前几天做了一个论文的博客，就提到transformer是这一代硬件的天选架构。对，穿穿了他肯定是天选。比方说他当时他设计出来说完了就是为了让他硬件快，设计出来了吗？像FFN那肯定不用说了，都是大举证惩罚，那肯定快。
发言人  
然后像attention，他其实就是之前attention之前大家是用LISTM这种IN这种不能并行的模块来做的。然后像STM他肯定硬件加速就更难搞。然后像attention他就算他是平方的复杂度，他复杂度比比安安高了一个级别，但是他就是可以通过矩阵乘法然后来算到那个oppo，所以它的硬件亲和是比要好很多的。所以大家会宁愿去用理论复杂度更高的这个transformer，也不会来用这个理论复杂度更低的这个STM。因为他们这个硬件清和表现完全不一样。
发言人  
对，然后我觉得算法整体发展就是要找到这一些硬件亲和，然后有更好的一些算法。因为transform它不但那个attention它不但是硬件更亲和，他确实也解决了一些长城依赖的关系的问题。对，所以他才会流行的这么开。像今天的话题又要tension又重新登上舞台，那肯定也离不开这一系列的发展。
发言人  
就比方说像那些把它分成trunk的那些并行的算法，然后他他更强的那些设计，能让他从machine learning performance这个角度来卖更加make sense这些嗯这些才是能推动他发展的原动力。所以我还是非常主张来做一些一些非常principle，支撑machine learning perspective来说，它是很principle很有他会mathematically grounded。然后就会有就是从数学上它是make sense的。就比方说德大儒他从事人生make sense，然后同时hardware alive的一些模型。对，因为我觉得做模型还是肯定要结合当前硬件的。
发言人  
就会有些人说我设计一个算法它足够好，那硬件公司来帮我优化那怎么可能呢？那你这三法你是经金子做的还是银子做的？能让硬件公司来天天帮你友好是不可能的。
发言人  
那你肯定你要首先你要让你的算法先去满足一些非常通用的原则。像hardware它有一些principle，像memory hierarchy这种东西，然后像举证乘法更有。像这种东西的话就是你不论看不同类型的hardware，它基本上都是会遵从这种原则的。就有一些universe的一些principle，对你说一下算法，你可能没必要去专门比方说针对h one hundred去优化。但我觉得中山法至少要去满足这些硬件比较通用的这种principle。要不然我觉得做的算法就是在当今这个scalability左右你的这个场景下面，基本上就是没有什么实际价值的，就纯自娱自乐。
发言人  
对，听力linear这个为硬件有做什么样的优化没有？可以明天要。我觉得他的那个算法还是硬件亲和的。然后kernel的话它现在应该还是张宇写的那个threaten的算法。对，就超好用。对我相信大家都没有那么多。Kernel优化它是一个非常耗时的一个工种，就非常的非常的需要时间。对，要慢慢磨，就是要老师傅课程优化慢慢的打磨。对我觉得就是aquatic迭代的时候，大家愿意用try turn写一下，凑合用就行了。然后如果他验出来有用的话，那后面来补一些扩大客流也是可以的吗？
发言人  
对，我觉得他们现在应该还在用Christmas科友在汛。对，我不知道他们后面会不会找人来一些从硬件亲和的角度，你觉得下一代的算法会怎么演进？现在我觉得阴茎眼镜的话，他跟transformer他是有一点协同，眼睛就是硬件会变成transformer更喜欢的模样。对，所以其实对于一些alternative来说，是有一些不好的那种因素在里面的。因为现在要架构这样硬件，大家会发现他就是为了去优化矩阵程，然后让他举证程越快越好。因为transfer里面有大量的举证权，他就想他就想硬件市场搞一些快速的举证权的东西。就比方说像tensor core，然后像这种TMA这种东西，然后像最近的black whale上面，它有一些专门针对这种举证层，它有一些单独的那种内存上面单独的那种memory，这都是来优化举证成的吗？对，所以可能大家会看到flash attention会越来越快，FA4它会在black White上面会越来越快。
发言人  
对，然后我觉得既然这个硬件是这么evolve的，那从设计算法的角度来看，那你就必须要设计一些能有举证惩罚的算法，要不然你这个硬件效率肯定是跟不上的对，像linear tension，他创个算法有个好处，就是他基本上都是一些举证层，当然他还会有一些其他的overhead。对，那这个的话可能就是得克服一下。对他可能比方说在training的时候，可能还是不如flash attention 4这种在black well上面搞笑。但其实也无所谓，有很多地方也不care训练效率，他只是care那种influence效率。
发言人  
所以我觉得是要训练的时候，就是能能以reasonable的速度来然后reasonable的速度来prefer。然后decoding快的话，这种架构其实也是有市场的对。然后另外的话就比方说像反馈MOE，然后sparse attention这种降flop，然后又能用矩阵乘法。对，他们都是属于这种类型，他们肯定还是要用举证乘法的。然后就是想办法把flops打下去，然后通过一些算法的这种创新来把flops打下去。
发言人  
对，然后同时保证这里面有大量的举证层，就一旦用算法里面基本上都举证成，那基本上这个算法也hardware也挺相对而言还是挺挺好优化的。对，因为我觉得这个硬件就是奔着王局正惩罚越来越快的方向在走了。就甚至说像像FA4就是举证成太快了，导致他那个so nice of mass那个is financial的那个模块都变成一个瓶颈了。对，然后他们就用一些FS的话，他就用一些approximate方法来做那个EXP。那这个也挺挺好笑的，就是举证成太快了。要让我们现在还是要去尽量去利用举证呈快速的这个性质。
发言人  
然后具体再一些算法，我觉得像deep sick sparse attention，他那个就已经用到了这种性质。我觉得他是一个非常注重这种硬件和算法协同设计的一个公司。像difficult pass attack的话，它会有一个叫index，它就是FP8来做这个算这个tennis school吗？
发言人  
因为他不需要soft max，他只需要上那个logic，然后来做top k来选score。所以首先它就是FP8，然后它又可以把昂贵的exponential的操作把它去掉。这样的话他就基本上就是一大堆矩阵城，然后他们所以说他那个那他那个index的话就会非常的快。这样的话就有可能就是能不能用到他们的下一代的架构吗？我也不知道下一代架构是什么，但这些性质在那里就有可能可以作为一个下一代架构的一个candidate。
发言人  
对，相对来说deep sick和kimi哪个在硬件清河上做的更好？听起来是deep sick，absolutely ok因为没有把这个作为一个重要的优化目标对吧？不太确定，我觉得kimi肯定还是scale up这种硬件上面的东西，但是没有deep seek那么追求。
发言人  
对我觉得他们非常追求之中。比方说这个算法能不能在FP上面，FP8上面跑这种之类的对不对？我觉得他们info应该在他们算法迭代的过程中，应该话语权会比较高一点。对我觉得这个都是因公司而异的。就有些公司他语法的话语权会高一些，有些公司他算法的话就会高一些，感觉算法就经常会搞一些让infrastructure的东西出来。
发言人  
你觉得如果年轻的研究者想要进入助力机制或者架构算法这些领域的话，你对他们有什么建议？他们应该是从哪些地方开始入手？现在的话找个公司去实习，ok就是跟上。因为因为我觉得因为我觉得坐架构必须要算力，没有算力就没法做架构。对，所以我觉得还是先找个level去实习。对嗯。
发言人  
好了，今天的节目就是这样。这里是商业访谈录，是一档由语言及世界工作室出品的深度访谈节目。也可以到公众号关注我们的工作室，获取更多的信息。我们的公众号是语言及世界，language is world, 我们希望和你一起从这里探索新的世界。
