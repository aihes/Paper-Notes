# Thoughts by a non-economist on AI and economics

[November 4, 2025November 4,
2025](https://windowsontheory.org/2025/11/04/thoughts-by-a-non-economist-on-
ai-and-economics/) ~ [Boaz
Barak](https://windowsontheory.org/author/windowsontheory/)

_Crossposted
on[lesswrong](https://www.lesswrong.com/posts/QQAWu7D6TceHwqhjm/thoughts-by-a-
non-economist-on-ai-and-economics)_

> _Modern humans first emerged about 100,000 years ago. For the next 99,800
> years or so, nothing happened. Well, not quite nothing. There were wars,
> political intrigue, the invention of agriculture ‚Äî but none of that stuff
> had much effect on the quality of people‚Äôs lives. Almost everyone lived on
> the modern equivalent of $400 to $600 a year, just above the subsistence
> level ‚Ä¶ Then ‚Äî just a couple of hundred years ago, maybe 10 generations ‚Äî
> people started getting richer. And richer and richer still. Per capita
> income, at least in the West, began to grow at the unprecedented rate of
> about three quarters of a percent per year. A couple of decades later, the
> same thing was happening around the world.‚Äù_ [Steven
> Lundsburg](https://www.wsj.com/articles/SB118134633403829656)

METR has had a very influential work by Kwa and West et al on [measuring AI‚Äôs
ability to complete long tasks](https://metr.org/blog/2025-03-19-measuring-ai-
ability-to-complete-long-tasks/). Its main result is the following remarkable
graph:

[![](https://windowsontheory.org/wp-
content/uploads/2025/11/image-1-1.png?w=1024)](https://windowsontheory.org/wp-
content/uploads/2025/11/image-1-1.png)

On the X axis is the release date of flagship LLMs. On the Y axis is the
following measure of their capabilities: take software-engineering tasks that
these models can succeed in solving 50% of the time, and measure the time it
takes _humans_ to solve them.

While it is not surprising that models improve over time, the main reason this
graph is remarkable is because the Y axis is on a log scale. This means that
there is a fixed period of time after which models have _doubled_ the length
of tasks they can complete successfully. Specifically METR estimates this
‚Äúdoubling time‚Äù (which is proportional to the inverse of the slope of the line
in this graph) at about 7 months, although they note that it may have
accelerated recently (to as little as 3 months if considering only models
after 2024). In this blog, just to make things simple, I will assume doubling
time is 6 months, so the length of time horizon quadruples every year. (All
numbers here are very rough.)

There are many factors that could potentially impact these results (and METR
did a pretty admirable job of enumerating them). It is worth separating them
into factors that impact the **intercept** (the actual length of time horizon
of tasks that models can do) and factors that impact the **slope** (the
doubling time) or possibly even the **shape** (e.g. breaking the linear
relation between model release time and log horizon).

### Factors impacting the intercept

The precise values ‚Äî e.g., GPT5 doing tasks that take humans 2 hours and 17
minutes ‚Äî could be impacted by numerous factors:

  * **Reliability factor (‚Üì)** ‚Äì the graph is plotted at 50% reliability. For 80% reliability METR got a similar slope (that is about the same doubling time) but the intercept is much lower, e.g. GPT5 can only perform tasks that take 26 minutes. (See also note on 100% accuracy below.)  

  * **Task type (‚Üï)** ‚Äì the graph is plotted for a specific benchmark. METR also studied [other domains](https://metr.org/blog/2025-07-14-how-does-time-horizon-vary-across-domains/). The data is more sparse but broadly concurs with the straight line fit (sometimes an even steeper line, though some of these datapoints are only available for more recent models where the slope is steeper).  

  * **‚ÄúBenchmark bias‚Äù** **(‚Üì)** ‚Äì AI models tend to do much better in tasks that are well scoped and easy to measure success in, while real world tasks are much ‚Äúmessier‚Äù ‚Äì in their specification, the context needed to solve them, and the way we measure success. It is still an open question if this impacts only the intercept, also the slope, or even the shape of the curve. ‚ÄúField experiments‚Äù are still very limited and with mixed results. However, the rapid rise in actual usage of AI is an indication that models‚Äô capabilities are not just limited to lab setting.Personally I am sure that this has a significant impact on the ‚Äúintercept‚Äù ‚Äî models pay a significant ‚Äúmessiness tax‚Äù in translating their performance from benchmarks to the real world ‚Äî but I am not convinced it impacts the slope. That is, the ‚Äúmessiness tax‚Äù may well be a fixed constant c<1 multiplier on the duration of tasks that models can handle.

## Factors impacting the slope/shape

  * **Exponential inputs (‚Üì):** The X axis of time for model release combines a number of inputs to models that have been growing at a rapid rate. These include compute, staff at AI labs, data, capital investment in AI, and more. For example [Epoch AI estimates](https://epoch.ai/data/ai-models) that the training compute for models has been doubling every 6 months. If the base of this exponent changes in one way or the other, this will likely impact the rate of progress. In particular, tautologically, sustaining exponential growth in inputs becomes exponentially more challenging over time‚Äìand quickly becomes impossible. Tha/t said, so far investment in these resources is showing no signs of slowing down.  

  * **New data / active learning (‚Üì):** So far by and large LLMs have been training on data produced by humans. One comparison can be to a human student, who throughout K-12 and undergraduate studies mostly learns from textbooks ‚Äì knowledge that has already been collected. However, in many professions, and in particular in science and inventions, humans need to discover new knowledge and collect new observations from the real world. If LLMs require inputs such as running scientific experiments or acting in the world to improve their intelligence, that would slow progress down. That said, so far we have not seen any such slowdown even as LLMs are becoming more ‚Äúagentic‚Äù. Indeed, the METR data is only showing a speedup in the slope in recent years.  

  * **Physical tasks and data (‚Üì):** METR primarily focused on software engineering. It is possible these trends will extend to other kinds of cognitive labor. But we do not know if they extend to domains where people need to either act in the physical world, or collect data from the physical world. While robotics has been [advancing](https://www.physicalintelligence.company/blog/pi05) as well, it is unclear whether or not it follows a similar exponential curve. Even if state of the art robotics improve at a similar rate to state of art models, manufacturing robots at scale can remain a bottleneck.  
  
I personally do not believe in a ‚Äúdata wall.‚Äù I believe that there are
diminishing returns to more data of the same type, and the advances over the
last few years, as captured by the METR graph, have largely not been due to a
larger quantity of data from the Internet. Also, while so far much of AI‚Äôs
economic impact has been in software engineering, it is telling that the best
models for software engineering are ones that are trained on very general
data, and are often good at [many other
tasks](https://www.lennysnewsletter.com/p/everyone-should-be-using-claude-
code) (while this article is about Claude code,I can‚Äôt help but note that in
my experience [codex](https://chatgpt.com/features/codex) too is great for
non-coding tasks üôÇ ). In my view, thinking that AI‚Äôs impact would be
restricted to software engineering is analogous to thinking in January 2020
that Covid‚Äôs impact would be restricted to China.**  
**

  * **Threshold effects (‚Üë):** These tasks are measured with respect to humans that have certain time scales in which they operate on. Us humans need to sleep, take breaks, and split work between workers, which requires isolating the context needed for completing a task and the way to measure its success. Hence we break tasks into ones that can be achieved in a working day, week, month, quarter. (I believe that even Andrew Wiles, who worked on Fermat‚Äôs last theorem for 7 years, broke it down to multiple intermediate results.) Thus it is possible that once AI reaches a certain time horizon, either (a) the measurement no longer makes sense, or (b) it can essentially simulate any combination of humans working for an arbitrarily large amount of time.  

  * **Recursive self improvement (‚Üë):** An important input to the process of producing AI models has been the work of human researchers and engineers. If AI itself can produce that input, then it could correspond to a massive increase in this input. It is still unknown whether using AI for automating the AI creation process will lead to a singularity, increase in slope, one-time boost, or just help sustain the exponential.

The bottom line is that we have significant uncertainty about the intercept,
but arguably less about the slope, or at least the general shape of it up to
the point of recursive self improvement / full automation of all human
cognitive labor. So from now on I will just assume that time horizons will be
doubling every 6 months and make no assumptions about the actual values of the
time horizons themselves.

## Sigmoidal relationship

Another remarkable figure in the METR
[paper](https://arxiv.org/abs/2503.14499) is the following (Figure 5 there):

[![](https://windowsontheory.org/wp-
content/uploads/2025/11/image-2.png?w=1024)](https://windowsontheory.org/wp-
content/uploads/2025/11/image-2.png)

What I find striking about it is how good is the ‚Äúsigmoid fit‚Äù for the
performance of models as a function of the time duration it takes a person to
do the task. In particular it seems that there is a certain threshold of time
duration such that below it, models essentially are successful 100% of the
time. This suggests that even the ‚Äú100% horizon‚Äù (as hard as it is to measure
empirically) will also double at a similar rate.

One way to think about this relation is that each task has a notion of
‚Äúdifficulty‚Äù (captured by log horizon length) and models have a certain
‚Äúskill‚Äù level that corresponds to how likely they are to succeed. In this
sense, this is similar to the [ELO
scale](https://en.wikipedia.org/wiki/Elo_rating_system) ‚Äì we can think of the
log horizon as the ‚ÄúELO rating‚Äù of the task, and a model will ‚Äúwin‚Äù against
the task with 50% chance if it has the same ELO rating. (Thanks to [Jacob
Hilton](https://www.lesswrong.com/posts/iEkksmar6mtuQtdsR/jacob_hilton-s-
shortform#FwkZAekBjZEvgzkBP) for this analogy.) For what it‚Äôs worth, here is
the chess ELO rating of models over time (as well as of humans), which
displays a similar linear growth over time (though with a long plateau from
around 1997-2007).

[![](https://windowsontheory.org/wp-
content/uploads/2025/11/image-1-2.png?w=1024)](https://windowsontheory.org/wp-
content/uploads/2025/11/image-1-2.png)

## Decreasing costs

Another striking statistic has been the [rapid
reduction](https://epoch.ai/data-insights/llm-inference-price-trends) in
prices for inference. That is, while it is often costly to extend the frontier
of intelligence, once we achieve a certain level X, the cost to provide the
same level has been decaying by factor 10x or more per year. It seems that
once we reach a certain frontier the first time, it is much cheaper and easier
to reach it for the second time (see also the ‚Äúdeepseek moment‚Äù). If this
trend continues, it might mean that once a job is automated, within a year the
cost for AI to do it will become insignificant.

[![](https://windowsontheory.org/wp-
content/uploads/2025/11/image-1-3.png?w=1024)](https://windowsontheory.org/wp-
content/uploads/2025/11/image-1-3.png)

Discussions on AI often assume that robotics would be an exception, where we
would not see as much progress. There are reasons to assume that due to costs
of production, and the inability to deploy robots as flexibly as we can
virtual AI assistants, costs for robotics would not be decreasing at the same
rate. It is less clear to me that there is a fundamental reason that the
‚Äúdoubling time‚Äù ‚Äì growth in complexity of tasks that robots can perform ‚Äì
would grow much slower than for other AI systems, but as far as I know there
is no data on this point.

## Implications for GDP growth

This is where I am getting to be pretty out of my depth, but I do want to see
if there are ways to make back of the envelope calculations. Because I am
focused on ‚Äúslope‚Äù vs ‚Äúintercept‚Äù, I will not try to predict so much _when_ AI
will lead to a certain growth but rather how fast we would get from the point
of significant growth to explosive growth. For example, how many years would
it take from the point that AI contributes to a 5% total growth in GDP to the
point when due to AI GDP has doubled.

I am focusing in this post only on growth, and not considering employment
outcomes. It is possible to construct some ‚Äúknife edge‚Äù scenarios under which
human displacement perfectly balances out productivity contributions of AI,
leading to decreased employment without increase in growth. But I don‚Äôt
believe such scenarios can be sustained under our assumptions of exponential
growth in capability and decrease in costs. Hence, assuming above trends
continue, significant labor impacts of AI will have to be coupled with
significant productivity gains.

[![](https://windowsontheory.org/wp-
content/uploads/2025/11/image-2-1.png?w=1024)](https://windowsontheory.org/wp-
content/uploads/2025/11/image-2-1.png)

A remarkable fact is that (adjusted for inflation) U.S. GDP per capita has
been growing at essentially a constant rate of roughly 2% over the past 150
years. None of the inventions in this period‚Äî including electrification,
internal combustion engine, computers and the Internet‚Äî changed this
trajectory. Note that 2% growth corresponds to a GDP ‚Äúdoubling rate‚Äù of 35
years.

Other countries have seen more significant fluctuations in growth (e.g. see
Japan) though it seems that it is easier to grow at a rapid rate when you are
far from the frontier, but once you reach it you slow down. This makes sense
from the point of view that advancing the frontier requires inventing new
ideas, while advancing to the frontier only requires copying and adapting
existing ideas.

Still it is interesting that GDP growth has been only 2% (or maybe around 3%
if we don‚Äôt consider per capita) even though Moore‚Äôs law corresponds to a
growth rate of about 40% per year. One explanation is ‚ÄúBaumol‚Äôs cost disease‚Äù
‚Äì while computers have been getting far more productive, people have been the
bottleneck (see also section II.C
[here](https://www.kellogg.northwestern.edu/faculty/jones-
ben/htm/A%20Framework%20for%20Economic%20Growth%20with%20Capital-
Embodied%20Technical%20Change.pdf)) . Another
[explanation](https://web.stanford.edu/~chadj/IdeaPF.pdf) is that good ideas
are getting harder to find, and hence it requires an increasingly large
technological input to get additional output.

The trillion dollar question is whether AI will break the 2% trend or not.
Will AI be just another technology that allows us to sustain 2% growth for
another couple of decades? Or will the ‚ÄúAI moment‚Äù be for us like post-WWII
Japan? That is, should we model it as if we are meeting a new ‚ÄúAI frontier
economy‚Äù which is vastly more productive, and this interaction will enable
rapid growth, with GDP at least doubling every decade as was the case for
Japan.

To put things in perspective, [Acemuglu](https://shapingwork.mit.edu/wp-
content/uploads/2024/05/Acemoglu_Macroeconomics-of-AI_May-2024.pdf) predicts
AI driven GDP growth to be about 0.1% per year while [Goldman
Sachs](https://www.goldmansachs.com/insights/articles/generative-ai-could-
raise-global-gdp-by-7-percent) predicts about 1.5% over a year. GDP doubling
over a decade would correspond to roughly 7% growth per year, or AI
contributing around 5% additional growth ‚Äì more than triple the Goldman Sachs
estimate, and 50 times higher than Acemuglu‚Äôs estimates. The consequences of
even a ‚Äúminor‚Äù boost in growth can be massive. A 1.2% increase in GDP growth
would be sufficient to put the U.S. economy on fiscally sustainable footing,
with no need for increased taxes or decreased spending, while a 2% increase in
GDP growth would be unprecedented for the U.S.

AI can contribute to GDP growth by either enabling replacement of labor with
capital, or increasing total factor productivity. Specifically, some versions
of [endogenous growth
theory](https://web.stanford.edu/~chadj/JonesHandbook2005.pdf) stipulate that
productivity grows with the production of ideas, and this production is in
turn monotonically (though [not
linearly](https://web.stanford.edu/~chadj/IdeaPF.pdf)) related to the number
of researchers/inventors.

If AI contributes via automating a specific industry, then the maximum benefit
to the GDP is bounded by the share of this industry. Specifically, if an
industry amounts for x fraction of the economy then (according to the model of
B. Jones, see below) automating it fully can boost GDP by at most 1/(1-x).
E.g., full automation of the software industry can increase the GDP by 1/0.98~
2%. Depending on how you measure it,[ cognitive
labor](https://shs.cairn.info/revue-travail-et-emploi-2019-1-page-45?lang=en)
arguably amounts to at least 30% of GDP (e.g. half of labor share of GDP) and
so full automation of it could increase it by 1/0.7 ~ 42%. If the latter
happens over a decade that would already correspond to 3.5% annual growth.

However, if AI also improves productivity of other industries (e.g. by
discovering new ideas) then the contribution can grow even beyond those that
are not directly automated. To be clear, to the extent AI contributes to
research and development, I expect this would be by accelerating science and
making scientists, researchers, and inventors more productive. This means that
in the coming years, funding human scientists will offer an even higher return
on investment than it has in the past.

Below I offer some vague intuitions and unfounded extrapolations to try to get
at how much and how fast we could expect AI to contribute to growth.

Note that there are [critiques of GDP](https://www.oecd.org/en/topics/policy-
issues/well-being-and-beyond-gdp.html) as a measure of progress, and various
alternative measures have been proposed. However, many of these are also at
least loosely correlated with GDP, with a spike in one corresponding to a
spike in the other. E.g., see this chart of the [augmented human development
index](https://ourworldindata.org/grapher/augmented-human-development-index)
of the U.S. and Japan.

[![](https://windowsontheory.org/wp-
content/uploads/2025/11/image-2-2.png?w=1024)](https://windowsontheory.org/wp-
content/uploads/2025/11/image-2-2.png)

While I am sure many of AI‚Äôs impacts will not be captured by the GDP, I
believe that if it will be truly as transformative as the industrial
revolution, this will show up in the GDP as well.

## Intuition from METR tasks

Let us make the (very rough and not really justified) assumption that in a
certain industry or job, the tasks are ‚Äúheavy tailed‚Äù where the fraction of
tasks that take a person at least T time to complete shrinks proportionally to
1/T. In this case, the time horizon is proportional to the fraction of tasks
that have not been automated. This means that the ‚Äúdoubling time‚Äù is the same
as the ‚Äúhalving time‚Äù of the fraction of tasks that have not been automated
yet.

Another way to think about it is by thinking of the ‚ÄúELO analogy‚Äù ‚Äì assume the
distribution of ‚ÄúELO ratings‚Äù of tasks is such that the fraction of tasks with
an ELO rating more than E is roughly exp(-E). (For chess players the
distribution of ELO scores is roughly
[normal](https://arxiv.org/pdf/1103.1530), which would be ~ exp(-E¬≤).)

If we assume a ‚Äúhalving time‚Äù of 6 months, it would take about 2 years from
the point of time that AI‚Äôs automates half of the tasks in some industry, to
the point in time that they automate 31/32 ~ 97% of the tasks in it.

This assumption is extremely aggressive in the sense that it is focused on
**capabilities** and ignores **diffusion**. It is possible that AI could
theoretically automate a large fraction of tasks but for many reasons it would
not automate them in practice. (Though it is also possible that diffusion will
progress unevenly, with sudden unlocking of latent capabilities.) It is also
important to note that this intuition runs counter to broad trends in
automation over the last 80 years which have historically been **linear** ‚Äì
with the share of automated tasks growing slowly and steadily, with a single-
digit percent rate of automation that is often declining over time. See the
following charts from C. Jones and Tonetti (2025), via ChatGPT:

[![](https://windowsontheory.org/wp-
content/uploads/2025/11/image-3.png?w=1024)](https://windowsontheory.org/wp-
content/uploads/2025/11/image-3.png)

Hence if AI will cause an exponential decay of the share of tasks performed by
labor, it would be a break with previous trends in automation and very
different from what we have seen in the past.

## AI as increasing population

One way to view AI is as injecting to the economy each year **t** a number
**N(t)** of new ‚Äúworkers‚Äù that have a certain quality **Q(t)** ‚Äì quality could
correspond to the fraction of economically useful tasks these workers can do,
so can correspond to both ‚Äúyears of schooling‚Äù and the generality of their
ability. We can define Q(t) as one over the fraction of tasks that have not
yet been automated in the corresponding industry.

Algorithmic advances in AI and the overall compute budget will determine the
mixture of workers and quality. As a point of comparison, the U.S. population
has been growing at about 2m per year due to natural growth and immigration,
and the U.S. labor force is about 160m people.

At a rough level, if capital and productivity is fixed, and we assume a Cobb-
Douglas production function with GDP being proportional to K·µÉL¬π‚Åª·µÉ with a=0.4
then increasing the labor force by a factor of C will increase the GDP by a
factor of C‚Å∞·êß‚Å∂. If AI creates 10m new virtual employees, that would increase
the GDP by a factor of (170/160)‚Å∞·êß‚Å∂ which is about 4 percent.

If it is 50m new employees this would be about 18%, and if AI doubles the
workforce then the growth would be ~50%. Of course, the number of AI ‚Äúvirtual
workers‚Äù could potentially be orders of magnitude more, at which point these
calculations are likely not to make much sense. (There is a book called ‚Äú[One
Billion Americans](https://en.wikipedia.org/wiki/One_Billion_Americans)‚Äù but I
think even its author did not ponder what would happen if there were one
trillion Americans‚Ä¶)  
  
It is hard to give predictions on either N(t) or Q(t), but it seems reasonable
to expect that both will grow exponentially. At the extreme, if we fix the
quality Q then we could expect N(t) to increase 10x per year (as costs for a
fixed model quality have decreased.) But perhaps a starting point assumption
would be that the product of both will grow at the 4x/year METR rate, with
both Q and N doubling every year. (Note that N(t) is the number of **new**
workers, and so is equal to TN(t)-TN(t-1) where TN(t) is the total number of
AI workers at year t, however since the derivative of an exponential is
exponential then it does not matter, especially if the base of this
exponential is 2.)

Such growth is sufficient to ensure that once AI starts providing a non-
trivial number of new workers (e.g., 100K workers in the U.S.) then within a
decade, AI will be the dominant source of labor in the U.S.

## Substitution and automation effects

A simplified version of the model of [B. Jones
(2025](https://www.kellogg.northwestern.edu/faculty/jones-
ben/htm/Artificial_Intelligence_in_Research_and_Development.pdf)) is that the
impact of AI on productivity is determined by the _harmonic mean_ of the
automatable and non automatable tasks.

Imagine that a œÅ fraction of R&D tasks cannot be automated, and 1-œÅ fraction
of tasks can be automated and be carried out at far lower cost than humans.
For simplicity, let‚Äôs assume that the human productivity in non-automated
tasks is 1, and the AI productivity in automated tasks is Œª>>1.

Then the improvement in productivity due to automation can be modeled as the
Harmonic average of 1 and Œª with weights œÅ and 1-œÅ respectively, that is:

(œÅ/1 + (1‚àíœÅ)/Œª)‚Åª¬π

Note that unlike the arithmetic or geometric means, even if Œª is infinite, the
maximum that this can reach is 1/œÅ. This makes sense if we think of tasks as
non-substitutable and so the maximum productivity gain we can get is if we
automated tasks that take humans 90% of the time, so that now one worker can
do 10x time the work per time unit.

Jones shows that in such a model, in order to get significant productivity
gains, both œÅ must shrink and Œª must grow. If one of them stays ‚Äústuck‚Äù then
so will productivity. Jones has the following graph of the regimes where we
can get ‚Äútransformative AI‚Äù in the sense of 10x productivity growth, which
would be similar to the scale of the productivity increase via the industrial
revolution. (If I am following the math correctly, the condition under Jones‚Äô
assumptions for this graph is that 9/[(2‚àíœÅ)/‚àö Œª + 2œÅ]¬≤ ‚â• 10.)

[![](https://windowsontheory.org/wp-
content/uploads/2025/11/image.png?w=808)](https://windowsontheory.org/wp-
content/uploads/2025/11/image.png)

Figure 5 in [Jones](https://www.kellogg.northwestern.edu/faculty/jones-
ben/htm/Artificial_Intelligence_in_Research_and_Development.pdf): Regime of
Œª,œÅ that leads to transformative AI in harmonic mean case.

One way to think about this graph is what happens if we assume that, via the
mechanisms discussed above, every year we make progress in both œÅ (fraction of
tasks not yet automated) and Œª (productivity factor of AI in doing these
tasks).

As discussed above, it is a ‚Äúleap of faith‚Äù but arguably not completely crazy
to assume that at some point (maybe once we reach a threshold such as AIs
doing 8 hours of work) œÅ will shrink by a factor of 4x per year, while Œª will
increase by a factor of 10x per year. At this rate, within a year, we would
move from the lefthand corner of this graph (1,1) to the point Œª =10 , œÅ=¬º
which is almost at the TAI boundary. (Note that Jones assumes that half of the
tasks have already been automated, so it might take us a while to get to the
lefthand corner of the graph. Also, accounting for the ‚Äúideas getting harder
to find‚Äù effect could decrease the shrinking rate, see section 4.4 in the
paper.)  

The assumption on œÅ shrinking by a 4x factor is very aggressive and arguably
unrealistic. Perhaps a much smaller factor such as 1.1 is more reasonable,
which would correspond to automating a 1/1.1 ~ 9% of remaining tasks each year
(as opposed to 75% in the case of 4x shrinkage of œÅ). Here is a plot of how
many years from the (1,1) corner it will take for transformative AI as a
function of the fraction of tasks automated per year, for various values of
cost decrease (growth in Œª).

[![](https://windowsontheory.org/wp-
content/uploads/2025/11/image-4.png?w=1024)](https://windowsontheory.org/wp-
content/uploads/2025/11/image-4.png)

We see that as long as the rate is significant, we can get to transformative
growth within one to two decades. Note also that this is less sensitive to
decrease in costs (growth in Œª), which could bode well for automating manual
tasks.

The bottom line is that the question on whether AI can lead to unprecedented
growth amounts to whether its exponential growth in capabilities will lead to
the fraction of unautomated tasks itself decreasing at exponential rates.

  
**Acknowledgements** : Thanks to Bharat Chandar, Jason Furman, Benjamin Jones
and Chad Jones for comments and discussions on this post.

### Share this:

  * [ Click to share on X (Opens in new window) X ](https://windowsontheory.org/2025/11/04/thoughts-by-a-non-economist-on-ai-and-economics/?share=twitter)
  * 

Like Loading...

### _Related_

Posted in [Uncategorized](https://windowsontheory.org/category/uncategorized/)

![Unknown's
avatar](https://0.gravatar.com/avatar/c68b52511be55be1911e97f188c8f1e6d70d055644c4430878b827acb1f0efcf?s=60&d=identicon&r=G)

##  Published by Boaz Barak

[ View all posts by Boaz Barak
](https://windowsontheory.org/author/windowsontheory/)


